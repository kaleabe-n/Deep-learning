{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MrWyFXBBmx5O"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- two features\n",
        "- one hidden layer with two neurons with relu activation\n",
        "- 2 output classes with sigmoid\n",
        "- loss as mse\n",
        "- derivation of softmax\n",
        "- derivation of catagorical loss"
      ],
      "metadata": {
        "id": "3gHNSToLF-_9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U2RjB6Sy_8x5"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Previous implementations"
      ],
      "metadata": {
        "id": "MrWyFXBBmx5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense layer"
      ],
      "metadata": {
        "id": "7t0VknFUl01W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "\n",
        "    def __init__(self,n_features, n_neurons):\n",
        "        self.weights = torch.rand((n_features,n_neurons))\n",
        "        self.bias = torch.rand(n_neurons)\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        self.output = torch.matmul(inputs,self.weights) + self.bias"
      ],
      "metadata": {
        "id": "eF_U5ITIl5Ox"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy"
      ],
      "metadata": {
        "id": "R3OxS16Vl7gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Accuracy:\n",
        "    def __init__(self):\n",
        "        self.output = 0\n",
        "    def forward(self,y_pred,y_true):\n",
        "        if y_pred.shape != y_true.shape:\n",
        "            one_hot_notation = torch.zeros(y_pred.shape)\n",
        "            one_hot_notation[range(len(y_pred)),y_true] = 1\n",
        "        else:\n",
        "            one_hot_notation = y_true\n",
        "        correct_values = y_pred==one_hot_notation\n",
        "        correct_values = correct_values * one_hot_notation\n",
        "        self.output = torch.sum(correct_values) / len(y_pred)\n",
        "        return self.output"
      ],
      "metadata": {
        "id": "9S99HrwPl_Ze"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Simgoid"
      ],
      "metadata": {
        "id": "iqHM3KuzmASG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_Sigmoid:\n",
        "    def forward(self,input):\n",
        "        self.output = 1 / (1 + torch.exp(-input))\n",
        "        return self.output"
      ],
      "metadata": {
        "id": "BiO7oSuGmH9i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation ReLU"
      ],
      "metadata": {
        "id": "U1zsq6_0mLLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_ReLU:\n",
        "    def forward(self,inputs):\n",
        "        self.output = torch.max(inputs,torch.tensor(0))\n",
        "        return self.output\n",
        "\n"
      ],
      "metadata": {
        "id": "o4JlE9YjmNhX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss categorical"
      ],
      "metadata": {
        "id": "rXsxYKWSmPlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_Catagorical:\n",
        "    def forward(self,y_pred,y_true):\n",
        "        if y_pred.shape !=  y_true.shape:\n",
        "            y_true -= 1\n",
        "            one_hot_notation = torch.zeros(y_pred.shape)\n",
        "            one_hot_notation[range(len(y_pred)),y_true] = 1\n",
        "        else:\n",
        "            one_hot_notation = y_pred\n",
        "        loss = -torch.sum(one_hot_notation * torch.log(y_pred)) / len(y_true)\n",
        "        self.output = loss\n",
        "        return loss"
      ],
      "metadata": {
        "id": "zTAeDEaamuU2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Softmax"
      ],
      "metadata": {
        "id": "z_Dsvdexn1ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_Softmax:\n",
        "    def forward(self,inputs):\n",
        "        pow = torch.exp(inputs - torch.max(inputs, axis=1, keepdims=True)[0])\n",
        "        summ = torch.sum(pow,axis=1,keepdims = True)\n",
        "        ans = pow / summ\n",
        "        self.output = ans\n",
        "        return ans\n"
      ],
      "metadata": {
        "id": "mSIpEBCtn43U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "gCNQXMo9nwU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classification_Model:\n",
        "    def __init__(self):\n",
        "        self.layer1 = DenseLayer(2,2)\n",
        "        self.layer1_activation = Activation_ReLU()\n",
        "        self.output_layer = DenseLayer(2,2)\n",
        "        self.output_layer_activation = Activation_Sigmoid()\n",
        "        self.error = float('inf')\n",
        "\n",
        "    def forward_prop(self,inputs):\n",
        "        self.input = inputs\n",
        "        self.layer1.forward(inputs)\n",
        "        self.layer1_activation.forward(self.layer1.output)\n",
        "        self.output_layer.forward(self.layer1_activation.output)\n",
        "        self.output_layer_activation.forward(self.output_layer.output)\n",
        "\n",
        "    def calc_error(self,y_true):\n",
        "        if self.output_layer_activation.output.shape != y_true.shape:\n",
        "            one_hot_notation = torch.zeros(self.output_layer_activation.output.shape)\n",
        "            one_hot_notation[y_true] = 1\n",
        "        else:\n",
        "            one_hot_notation = y_true\n",
        "        self.y_true = one_hot_notation\n",
        "        self.error = (self.output_layer_activation.output - one_hot_notation)\n",
        "        self.mse = torch.mean(self.error)\n",
        "\n",
        "    def back_prop(self,lr):\n",
        "        dloss_by_dsig = self.error\n",
        "        dsig_by_layer2 = (torch.tensor([1]) - self.output_layer_activation.output) * self.output_layer_activation.output\n",
        "\n",
        "        # layer 2\n",
        "        back2 = dloss_by_dsig * dsig_by_layer2\n",
        "        layer2_grad = torch.cat((self.layer1_activation.output.unsqueeze(dim=0),self.layer1_activation.output.unsqueeze(dim=0)),dim=0).T * back2\n",
        "        self.output_layer.weights -= torch.tensor([lr]) * layer2_grad\n",
        "        self.output_layer.bias -= torch.tensor([lr]) * back2\n",
        "\n",
        "        # layer 1\n",
        "        drelu_by_dlayer1 = self.layer1.output>0\n",
        "        back1 = drelu_by_dlayer1 * torch.sum(self.output_layer.weights * back2,dim=1,keepdims=True).squeeze()\n",
        "        layer1_grad = torch.cat((self.input.unsqueeze(dim=0),self.input.unsqueeze(dim=0)),dim=0).T * back1\n",
        "        self.layer1.weights -= torch.tensor([lr]) * layer1_grad\n",
        "        self.layer1.bias -= torch.tensor([lr]) * back1\n",
        "\n",
        "\n",
        "    def fit(self,inputs,y_true,epoches=1000,lr=0.01):\n",
        "        for epoch in range(epoches):\n",
        "            for i in range(len(inputs)):\n",
        "                self.forward_prop(inputs[i])\n",
        "                self.calc_error(y_true[i])\n",
        "                if self.mse < 0.1:\n",
        "                    break\n",
        "                self.back_prop(lr)\n",
        "        error = torch.tensor([0,0],dtype = torch.float)\n",
        "        for i in range(len(inputs)):\n",
        "            self.forward_prop(inputs[i])\n",
        "            self.calc_error(y_true[i])\n",
        "            print(\"output:\",self.output_layer_activation.output,\"expected: \",self.y_true)\n",
        "            print(\"squared error\",self.error**2)\n",
        "            print()\n",
        "            error += self.error**2\n",
        "\n",
        "        avg_error = error / len(inputs)\n",
        "        print(\"average squared error\",avg_error)\n",
        "        print(\"over all mse:\",torch.mean(avg_error))\n",
        "\n"
      ],
      "metadata": {
        "id": "qitCcFqPophC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Classification_Model()\n",
        "inputs = torch.tensor([[1,2],[2,3],[5,6],[7,9],[7,8],[1,4]],dtype=torch.float)\n",
        "y_true = torch.tensor([0,0,1,1,1,0])\n",
        "model.fit(inputs,y_true,epoches=20000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa8KzfxnGhYX",
        "outputId": "c4d76942-da6e-4de2-e2bc-5e7fa9e7c532"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: tensor([0.6713, 0.5280]) expected:  tensor([1., 0.])\n",
            "squared error tensor([0.1080, 0.2788])\n",
            "\n",
            "output: tensor([0.6618, 0.4874]) expected:  tensor([1., 0.])\n",
            "squared error tensor([0.1144, 0.2376])\n",
            "\n",
            "output: tensor([0.6325, 0.3692]) expected:  tensor([0., 1.])\n",
            "squared error tensor([0.4001, 0.3979])\n",
            "\n",
            "output: tensor([0.6106, 0.2936]) expected:  tensor([0., 1.])\n",
            "squared error tensor([0.3728, 0.4990])\n",
            "\n",
            "output: tensor([0.6124, 0.2976]) expected:  tensor([0., 1.])\n",
            "squared error tensor([0.3750, 0.4934])\n",
            "\n",
            "output: tensor([0.6680, 0.5183]) expected:  tensor([1., 0.])\n",
            "squared error tensor([0.1102, 0.2686])\n",
            "\n",
            "average squared error tensor([0.2468, 0.3625])\n",
            "over all mse: tensor(0.3047)\n"
          ]
        }
      ]
    }
  ]
}